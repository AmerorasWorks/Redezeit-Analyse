{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d8143887836660",
   "metadata": {},
   "source": [
    "# CSV processing for Redezeit's dashboard scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af22fd9cab8a3f",
   "metadata": {},
   "source": [
    "This notebook's purpose is to process the scraped csv files from Redezeit's Looker Studio dashboard and rename the csv files with a more meaningful name.\n",
    "We preferred a nb instead of a .py file to have a controlled view of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.729124Z",
     "start_time": "2025-06-28T17:14:45.719853Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90caa5863a34e181",
   "metadata": {},
   "source": [
    "### Path, file list, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8956d85e672f5f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.749187Z",
     "start_time": "2025-06-28T17:14:45.743701Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Current nb location\n",
    "BASE_DIR = BASE_DIR = os.getcwd()\n",
    "\n",
    "#  Path to raw data folder\n",
    "folder_path = os.path.normpath(os.path.join(BASE_DIR, '..', 'data', 'raw'))\n",
    "\n",
    "#  Create output folder for cleaned CSV'set\n",
    "clean_folder = os.path.join(BASE_DIR, '..', 'data', 'clean')\n",
    "os.makedirs(clean_folder, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    'landingpage.csv',\n",
    "    'user_behaviors.csv',\n",
    "    'what_devices_used_chart.csv',\n",
    "    'what_did_user_do.csv',\n",
    "    'where_did_they_come_from.csv',\n",
    "    'where_new_visitors_come_from_chart.csv',\n",
    "    'who_was_visiting_chart.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Standardize final filenames\n",
    "\n",
    "final_names = {\n",
    "    'landingpage.csv':                        'landing_page_views.csv',\n",
    "    'user_behaviors.csv':                     'user_sessions.csv',\n",
    "    'what_devices_used_chart.csv':            'device_usage.csv',\n",
    "    'what_did_user_do.csv':                   'user_events.csv',\n",
    "    'where_did_they_come_from.csv':           'traffic_sources.csv',\n",
    "    'where_new_visitors_come_from_chart.csv': 'traffic_source_chart.csv',\n",
    "    'who_was_visiting_chart.csv':             'daily_visitors_chart.csv'\n",
    "}\n",
    "\n",
    "#  Expected numer of cols in each file (for validation)\n",
    "expected_columns = {\n",
    "    'landingpage.csv':                        3,\n",
    "    'user_behaviors.csv':                     3,\n",
    "    'what_devices_used_chart.csv':            4,\n",
    "    'what_did_user_do.csv':                   6,\n",
    "    'where_did_they_come_from.csv':           3,\n",
    "    'where_new_visitors_come_from_chart.csv': 6,\n",
    "    'who_was_visiting_chart.csv':             6\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd68a0af6ac1ce4",
   "metadata": {},
   "source": [
    "####  Helper: sort by week\n",
    "##### Not ready yet, overwritting everything atm bc i need then ratbros (advice bros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7983938e32bd5de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.765895Z",
     "start_time": "2025-06-28T17:14:45.759620Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def extract_week_folder(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the ISO calendar week folder name (e.g. '2025_KW07') based on the first valid\n",
    "    date found in the 'Datum' column of a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        str: Folder name representing year and calendar week, or \"unknown_week\" if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            # Read header line and remove Byte Order Mark (BOM) if present\n",
    "            header_line = f.readline().lstrip('\\ufeff')\n",
    "            header = header_line.strip().split(';')\n",
    "            \n",
    "            # Convert header names to lowercase for case-insensitive search\n",
    "            header_lower = [h.lower() for h in header]\n",
    "            \n",
    "            # Find the index of the 'datum' column (date column)\n",
    "            datum_idx = header_lower.index(\"datum\")\n",
    "            \n",
    "            # Iterate through each line in the file to find a valid date\n",
    "            for line in f:\n",
    "                columns = line.strip().split(';')\n",
    "                \n",
    "                # Skip line if it doesn't have enough columns\n",
    "                if len(columns) <= datum_idx:\n",
    "                    continue\n",
    "                \n",
    "                date_str = columns[datum_idx].strip()\n",
    "                \n",
    "                try:\n",
    "                    # Parse the date string assuming format YYYY-MM-DD\n",
    "                    dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                    \n",
    "                    # Extract year and ISO calendar week number\n",
    "                    year, week, _ = dt.isocalendar()\n",
    "                    \n",
    "                    # Format the folder name like '2025_KW07'\n",
    "                    week_folder = f\"{year}_KW{week:02d}\"\n",
    "                    \n",
    "                    # Confirm success once, then return the folder name\n",
    "                    print(f\"âœ… Week folder extracted: {week_folder} from file {path}\")\n",
    "                    return week_folder\n",
    "                \n",
    "                except ValueError:\n",
    "                    # If date parsing fails, try next line\n",
    "                    continue\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ File not found: {path}\")\n",
    "    except Valu\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c1c15",
   "metadata": {},
   "source": [
    "####  Helper: column count validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be50c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv(path: str, expected_cols: int):\n",
    "    \"\"\"\n",
    "    Scan a semicolon-delimited CSV and report any lines\n",
    "    that donâ€™t have exactly expected_cols fields.\n",
    "    Returns a list of (line_number, text) for the bad lines.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if line.count(\";\") + 1 != expected_cols:\n",
    "                issues.append((i, line.rstrip(\"\\n\")))\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d60dec775153e",
   "metadata": {},
   "source": [
    "### Morphin'-zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20040b75883ea6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:24:02.002234Z",
     "start_time": "2025-06-28T17:24:01.938901Z"
    }
   },
   "outputs": [],
   "source": [
    "cleaned_files = []\n",
    "\n",
    "for raw_fname in file_names:\n",
    "    raw_path = os.path.join(folder_path, raw_fname)\n",
    "    final_name = final_names.get(raw_fname, raw_fname)\n",
    "\n",
    "    # Ensure destination folder exists\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    clean_path = os.path.join(clean_folder, final_name)\n",
    "\n",
    "    # Copy raw file to clean folder\n",
    "    with open(raw_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(clean_path, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(infile.read())\n",
    "\n",
    "    # Validate cleaned file columns count\n",
    "    exp = expected_columns.get(final_name)\n",
    "    if exp:\n",
    "        problems = validate_csv(clean_path, exp)\n",
    "        if problems:\n",
    "            print(f\"âš ï¸ {final_name} has {len(problems)} malformed lines:\")\n",
    "            for ln, txt in problems[:5]:\n",
    "                print(f\"  line {ln}: {txt}\")\n",
    "\n",
    "    cleaned_files.append(clean_path)\n",
    "\n",
    "print(\"âœ… All files copied, renamed, and validated in clean folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f1089672c4939",
   "metadata": {},
   "source": [
    "### 5) Load CSV's into df\n",
    "\n",
    "By loading everything in df's, we make sure things are smooth and working, ready to load into the DB. Converting types here makes sense because this way we can catch errors before we load everything in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa942459875a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:32:19.374640Z",
     "start_time": "2025-06-28T17:32:19.285539Z"
    }
   },
   "outputs": [],
   "source": [
    "dataframes = {}\n",
    "for root, dirs, files in os.walk(clean_folder):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".csv\"):\n",
    "            path = os.path.join(root, fname)\n",
    "            key = final_names.get(fname, fname.replace(\".csv\", \"\"))\n",
    "            df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "            dataframes[key] = df\n",
    "            print(f\"âœ…{key} loaded from {path}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37557bae54da",
   "metadata": {},
   "source": [
    "### 6)  Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8843061463182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:54.572261Z",
     "start_time": "2025-06-28T17:44:54.561240Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_and_cast_columns(df):\n",
    "    \"\"\"\n",
    "    Cleans and casts DataFrame columns:\n",
    "    - Dates to datetime\n",
    "    - Durations to timedelta strings and numeric seconds/days\n",
    "    - Percentages to floats\n",
    "    - Numeric columns cast to int if possible, else float\n",
    "    - Preserve strings as is\n",
    "\n",
    "    Returns errors dict if any issues, else None\n",
    "    \"\"\"\n",
    "\n",
    "    errors = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        is_obj = df[col].dtype == \"object\"\n",
    "        sample = df[col].dropna().astype(str) if is_obj else None\n",
    "\n",
    "        # 1) Date conversion\n",
    "        if \"datum\" in col.lower():\n",
    "            converted = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'datetime_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 2) Duration hh:mm:ss\n",
    "        if is_obj and sample.str.match(r\"^\\d{2}:\\d{2}:\\d{2}$\").all():\n",
    "            td = pd.to_timedelta(df[col], errors=\"coerce\")\n",
    "            failed = td.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'timedelta_failures': failed.sum()}\n",
    "            df[col] = td.astype(str).str[-8:]\n",
    "            df[f\"{col}_seconds\"] = td.dt.total_seconds()\n",
    "            df[f\"{col}_days\"] = td.dt.total_seconds() / 86400\n",
    "            continue\n",
    "\n",
    "        # 3) Percentage conversion\n",
    "        if is_obj and sample.str.match(r\"^[\\d\\.\\,]+\\s*%$\").all():\n",
    "            stripped = sample.str.replace(\"%\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "            converted = pd.to_numeric(stripped, errors=\"coerce\") / 100\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'percentage_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 4) Numeric casting for numeric columns or numeric-like strings\n",
    "        if not is_obj:\n",
    "            # Numeric column - check if all integers (no decimals)\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                # Already int, no action needed\n",
    "                continue\n",
    "            elif pd.api.types.is_float_dtype(df[col]):\n",
    "                # Check if float column can be converted to int safely (no decimals)\n",
    "                if (df[col].dropna() % 1 == 0).all():\n",
    "                    df[col] = df[col].astype('Int64')  # nullable integer dtype to keep NaNs\n",
    "                # else keep as float\n",
    "                continue\n",
    "            else:\n",
    "                # Other numeric types, convert to float by default\n",
    "                df[col] = df[col].astype(float)\n",
    "                continue\n",
    "\n",
    "        # Try converting object columns with numeric values\n",
    "        if is_obj:\n",
    "            # Check if numeric-like strings\n",
    "            if sample.str.match(r\"^\\d+(\\.\\d+)?$\").all():\n",
    "                converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                failed = converted.isna() & df[col].notna()\n",
    "                if failed.any():\n",
    "                    errors[col] = {'numeric_failures': failed.sum()}\n",
    "                    df[col] = df[col].astype(str)  # fallback keep as string\n",
    "                else:\n",
    "                    # Cast to int if all whole numbers else float\n",
    "                    if (converted.dropna() % 1 == 0).all():\n",
    "                        df[col] = converted.astype('Int64')\n",
    "                    else:\n",
    "                        df[col] = converted.astype(float)\n",
    "                continue\n",
    "\n",
    "            # Otherwise preserve as string\n",
    "            df[col] = sample\n",
    "\n",
    "    if errors:\n",
    "        return df, errors\n",
    "    else:\n",
    "        print(\"âœ… All columns cleaned and cast successfully.\")\n",
    "        return df, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f8dcb7f96e511",
   "metadata": {},
   "source": [
    "### 7) Run them Jewells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec68df953cfc62ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:58.622623Z",
     "start_time": "2025-06-28T17:44:58.541023Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"ðŸ§™ðŸ½â€â™‚ï¸ ...morphin' DataFrame: '{name}'\")\n",
    "    cleaned_df, error_info = clean_and_cast_columns(df)\n",
    "    dataframes[name] = cleaned_df\n",
    "    if error_info:\n",
    "        print(f\"Errors in '{name}': {error_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2aacaf93ee925",
   "metadata": {},
   "source": [
    "### 8) Datatype check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca6e553b3aa9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:14.475854Z",
     "start_time": "2025-06-28T17:45:14.417200Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' data types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e85f2c306a863",
   "metadata": {},
   "source": [
    "### 9) Head check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338042d6c58b5ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:16.559945Z",
     "start_time": "2025-06-28T17:45:16.545189Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' head:\")\n",
    "    print(df.head())\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
