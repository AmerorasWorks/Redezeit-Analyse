{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d8143887836660",
   "metadata": {},
   "source": [
    "# CSV processing for Redezeit's dashboard scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af22fd9cab8a3f",
   "metadata": {},
   "source": [
    "This notebook's purpose is to process the scraped csv files from Redezeit's Looker Studio dashboard and rename the csv files with a more meaningful name.\n",
    "We preferred a nb instead of a .py file to have a controlled view of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.729124Z",
     "start_time": "2025-06-28T17:14:45.719853Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90caa5863a34e181",
   "metadata": {},
   "source": [
    "### Path, file list, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8956d85e672f5f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.749187Z",
     "start_time": "2025-06-28T17:14:45.743701Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Current nb location\n",
    "BASE_DIR = BASE_DIR = os.getcwd()\n",
    "\n",
    "#  Path to raw data folder\n",
    "folder_path = os.path.normpath(os.path.join(BASE_DIR, '..', 'data', 'raw'))\n",
    "\n",
    "#  Create output folder for cleaned CSV'set\n",
    "clean_folder = os.path.join(BASE_DIR, '..', 'data', 'clean')\n",
    "os.makedirs(clean_folder, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    'landingpage.csv',\n",
    "    'user_behaviors.csv',\n",
    "    'what_devices_used_chart.csv',\n",
    "    'what_did_user_do.csv',\n",
    "    'where_did_they_come_from.csv',\n",
    "    'where_new_visitors_come_from_chart.csv',\n",
    "    'who_was_visiting_chart.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Standardize final filenames\n",
    "\n",
    "final_names = {\n",
    "    'landingpage.csv':                        'landing_page_views.csv',\n",
    "    'user_behaviors.csv':                     'user_sessions.csv',\n",
    "    'what_devices_used_chart.csv':            'device_usage.csv',\n",
    "    'what_did_user_do.csv':                   'user_events.csv',\n",
    "    'where_did_they_come_from.csv':           'traffic_sources.csv',\n",
    "    'where_new_visitors_come_from_chart.csv': 'traffic_source_chart.csv',\n",
    "    'who_was_visiting_chart.csv':             'daily_visitors_chart.csv'\n",
    "}\n",
    "\n",
    "#  Expected numer of cols in each file (for validation)\n",
    "expected_columns = {\n",
    "    'landingpage.csv':                        3,\n",
    "    'user_behaviors.csv':                     3,\n",
    "    'what_devices_used_chart.csv':            4,\n",
    "    'what_did_user_do.csv':                   6,\n",
    "    'where_did_they_come_from.csv':           3,\n",
    "    'where_new_visitors_come_from_chart.csv': 6,\n",
    "    'who_was_visiting_chart.csv':             6\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c1c15",
   "metadata": {},
   "source": [
    "####  Helper: column count validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be50c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv(path: str, expected_cols: int):\n",
    "    \"\"\"\n",
    "    Scan a semicolon-delimited CSV and report any lines\n",
    "    that don’t have exactly expected_cols fields.\n",
    "    Returns a list of (line_number, text) for the bad lines.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if line.count(\";\") + 1 != expected_cols:\n",
    "                issues.append((i, line.rstrip(\"\\n\")))\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e0426",
   "metadata": {},
   "source": [
    "#### Helper: snake case column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03edfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_snake_case(col_name):\n",
    "    col_name = col_name.strip().lower()\n",
    "    col_name = re.sub(r\"[^\\w\\s]\", \"\", col_name)  # Remove special chars\n",
    "    col_name = re.sub(r\"\\s+\", \"_\", col_name)     # Replace whitespace with _\n",
    "    return col_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64f9cf",
   "metadata": {},
   "source": [
    "#### Helper: Time STR to seconds, append new columns to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24f7dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_str_to_seconds(time_str):\n",
    "    \"\"\"\n",
    "    Convert 'HH:MM:SS' or 'MM:SS' (or even 'H:MM:SS') into total seconds float.\n",
    "    Non‑parsable strings return NaN.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parts = [float(p) for p in time_str.split(':')]\n",
    "    except:\n",
    "        return float('nan')\n",
    "    if len(parts) == 3:\n",
    "        h, m, s = parts\n",
    "    elif len(parts) == 2:\n",
    "        h, m, s = 0, parts[0], parts[1]\n",
    "    else:\n",
    "        return float('nan')\n",
    "    return h*3600 + m*60 + s\n",
    "\n",
    "def append_time_columns_to_csv(csv_path):\n",
    "    # Load with semicolon\n",
    "    df = pd.read_csv(csv_path, sep=';')\n",
    "    new_cols = {}\n",
    "    \n",
    "    # Find any object‑dtype column whose sample values match a time pattern\n",
    "    time_pattern = re.compile(r'^\\d{1,2}:\\d{2}(?::\\d{2})?$')\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        sample = df[col].dropna().astype(str).head(10).tolist()\n",
    "        if all(time_pattern.match(s) for s in sample if s):\n",
    "            # Convert entire column\n",
    "            sec_col = f\"{col}_seconds\"\n",
    "            day_col = f\"{col}_days\"\n",
    "            df[sec_col] = df[col].apply(time_str_to_seconds)\n",
    "            df[day_col] = df[sec_col] / 86400.0\n",
    "            new_cols[col] = (sec_col, day_col)\n",
    "    \n",
    "    if new_cols:\n",
    "        # Overwrite CSV, preserving semicolon delimiter\n",
    "        df.to_csv(csv_path, index=False, sep=';')\n",
    "    return new_cols  # for logging if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d60dec775153e",
   "metadata": {},
   "source": [
    "### Morphin'-zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20040b75883ea6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:24:02.002234Z",
     "start_time": "2025-06-28T17:24:01.938901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ In user_sessions.csv, added time columns for: ['durchschn_zeit_auf_der_seite']\n",
      "✅ All files cleaned, headers normalized, time columns appended, and validated.\n"
     ]
    }
   ],
   "source": [
    "cleaned_files = []\n",
    "\n",
    "for raw_fname in file_names:\n",
    "    raw_path = os.path.join(folder_path, raw_fname)\n",
    "    final_name = final_names.get(raw_fname, raw_fname)\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    clean_path = os.path.join(clean_folder, final_name)\n",
    "\n",
    "    # 1) Copy & snake_case headers\n",
    "    with open(raw_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(clean_path, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        writer = csv.writer(outfile, delimiter=';')\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0:\n",
    "                writer.writerow([to_snake_case(col) for col in row])\n",
    "            else:\n",
    "                writer.writerow(row)\n",
    "\n",
    "    # 2) Append time-based columns\n",
    "    added = append_time_columns_to_csv(clean_path)\n",
    "    if added:\n",
    "        print(f\"ℹ️ In {final_name}, added time columns for: {list(added.keys())}\")\n",
    "\n",
    "    # 3) Validate\n",
    "    exp = expected_columns.get(final_name)\n",
    "    if exp:\n",
    "        problems = validate_csv(clean_path, exp)\n",
    "        if problems:\n",
    "            print(f\"⚠️ {final_name} has {len(problems)} malformed lines:\")\n",
    "            for ln, txt in problems[:5]:\n",
    "                print(f\"  line {ln}: {txt}\")\n",
    "\n",
    "    cleaned_files.append(clean_path)\n",
    "\n",
    "print(\"✅ All files cleaned, headers normalized, time columns appended, and validated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f1089672c4939",
   "metadata": {},
   "source": [
    "### 5) Load CSV's into df\n",
    "\n",
    "By loading everything in df's, we make sure things are smooth and working, ready to load into the DB. Converting types here makes sense because this way we can catch errors before we load everything in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4aa942459875a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:32:19.374640Z",
     "start_time": "2025-06-28T17:32:19.285539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅daily_visitors_chart loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\daily_visitors_chart.csv: (632, 3)\n",
      "✅device_usage loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\device_usage.csv: (2852, 3)\n",
      "✅landing_page_views loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\landing_page_views.csv: (19173, 4)\n",
      "✅traffic_sources loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\traffic_sources.csv: (8778, 6)\n",
      "✅traffic_source_chart loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\traffic_source_chart.csv: (8476, 3)\n",
      "✅user_events loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\user_events.csv: (19321, 6)\n",
      "✅user_sessions loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\user_sessions.csv: (1147, 8)\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "for root, dirs, files in os.walk(clean_folder):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".csv\"):\n",
    "            path = os.path.join(root, fname)\n",
    "            key = final_names.get(fname, fname.replace(\".csv\", \"\"))\n",
    "            df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "            dataframes[key] = df\n",
    "            print(f\"✅{key} loaded from {path}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37557bae54da",
   "metadata": {},
   "source": [
    "### 6)  Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaa8843061463182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:54.572261Z",
     "start_time": "2025-06-28T17:44:54.561240Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_and_cast_columns(df):\n",
    "    \"\"\"\n",
    "    Cleans and casts DataFrame columns:\n",
    "    - Dates to datetime\n",
    "    - Durations to timedelta strings and numeric seconds/days\n",
    "    - Percentages to floats\n",
    "    - Numeric columns cast to int if possible, else float\n",
    "    - Preserve strings as is\n",
    "\n",
    "    Returns errors dict if any issues, else None\n",
    "    \"\"\"\n",
    "\n",
    "    errors = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        is_obj = df[col].dtype == \"object\"\n",
    "        sample = df[col].dropna().astype(str) if is_obj else None\n",
    "\n",
    "        # 1) Date conversion\n",
    "        if \"datum\" in col.lower():\n",
    "            converted = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'datetime_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 2) Duration hh:mm:ss\n",
    "        if is_obj and sample.str.match(r\"^\\d{2}:\\d{2}:\\d{2}$\").all():\n",
    "            td = pd.to_timedelta(df[col], errors=\"coerce\")\n",
    "            failed = td.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'timedelta_failures': failed.sum()}\n",
    "            df[col] = td.astype(str).str[-8:]\n",
    "            df[f\"{col}_seconds\"] = td.dt.total_seconds()\n",
    "            df[f\"{col}_days\"] = td.dt.total_seconds() / 86400\n",
    "            continue\n",
    "\n",
    "        # 3) Percentage conversion\n",
    "        if is_obj and sample.str.match(r\"^[\\d\\.\\,]+\\s*%$\").all():\n",
    "            stripped = sample.str.replace(\"%\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "            converted = pd.to_numeric(stripped, errors=\"coerce\") / 100\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'percentage_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 4) Numeric casting for numeric columns or numeric-like strings\n",
    "        if not is_obj:\n",
    "            # Numeric column - check if all integers (no decimals)\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                # Already int, no action needed\n",
    "                continue\n",
    "            elif pd.api.types.is_float_dtype(df[col]):\n",
    "                # Check if float column can be converted to int safely (no decimals)\n",
    "                if (df[col].dropna() % 1 == 0).all():\n",
    "                    df[col] = df[col].astype('Int64')  # nullable integer dtype to keep NaNs\n",
    "                # else keep as float\n",
    "                continue\n",
    "            else:\n",
    "                # Other numeric types, convert to float by default\n",
    "                df[col] = df[col].astype(float)\n",
    "                continue\n",
    "\n",
    "        # Try converting object columns with numeric values\n",
    "        if is_obj:\n",
    "            # Check if numeric-like strings\n",
    "            if sample.str.match(r\"^\\d+(\\.\\d+)?$\").all():\n",
    "                converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                failed = converted.isna() & df[col].notna()\n",
    "                if failed.any():\n",
    "                    errors[col] = {'numeric_failures': failed.sum()}\n",
    "                    df[col] = df[col].astype(str)  # fallback keep as string\n",
    "                else:\n",
    "                    # Cast to int if all whole numbers else float\n",
    "                    if (converted.dropna() % 1 == 0).all():\n",
    "                        df[col] = converted.astype('Int64')\n",
    "                    else:\n",
    "                        df[col] = converted.astype(float)\n",
    "                continue\n",
    "\n",
    "            # Otherwise preserve as string\n",
    "            df[col] = sample\n",
    "\n",
    "    if errors:\n",
    "        return df, errors\n",
    "    else:\n",
    "        print(\"✅ All columns cleaned and cast successfully.\")\n",
    "        return df, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f8dcb7f96e511",
   "metadata": {},
   "source": [
    "### 7) Run them Jewells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec68df953cfc62ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:58.622623Z",
     "start_time": "2025-06-28T17:44:58.541023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧙🏽‍♂️ ...morphin' DataFrame: 'daily_visitors_chart'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'device_usage'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'landing_page_views'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'traffic_sources'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'traffic_source_chart'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'user_events'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'user_sessions'\n",
      "✅ All columns cleaned and cast successfully.\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"🧙🏽‍♂️ ...morphin' DataFrame: '{name}'\")\n",
    "    cleaned_df, error_info = clean_and_cast_columns(df)\n",
    "    dataframes[name] = cleaned_df\n",
    "    if error_info:\n",
    "        print(f\"Errors in '{name}': {error_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2aacaf93ee925",
   "metadata": {},
   "source": [
    "### 8) Datatype check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bca6e553b3aa9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:14.475854Z",
     "start_time": "2025-06-28T17:45:14.417200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'device_usage' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'landing_page_views' data types:\n",
      "datum          datetime64[ns]\n",
      "eid                     Int64\n",
      "seitentitel            object\n",
      "aufrufe               float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_sources' data types:\n",
      "datum                  datetime64[ns]\n",
      "eid                             Int64\n",
      "quelle                         object\n",
      "sitzungen                       int64\n",
      "aufrufe                       float64\n",
      "aufrufe_pro_sitzung           float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_source_chart' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_events' data types:\n",
      "datum              datetime64[ns]\n",
      "eid                         Int64\n",
      "name_des_events            object\n",
      "event_label                object\n",
      "aktive_nutzer               int64\n",
      "ereignisanzahl              int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_sessions' data types:\n",
      "datum                                   datetime64[ns]\n",
      "seitenaufrufe                                  float64\n",
      "nutzer_insgesamt                                 int64\n",
      "durchschn_zeit_auf_der_seite                    object\n",
      "absprungrate                                   float64\n",
      "seiten_sitzung                                 float64\n",
      "durchschn_zeit_auf_der_seite_seconds             Int64\n",
      "durchschn_zeit_auf_der_seite_days              float64\n",
      "dtype: object\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' data types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e85f2c306a863",
   "metadata": {},
   "source": [
    "### 9) Head check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "338042d6c58b5ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:16.559945Z",
     "start_time": "2025-06-28T17:45:16.545189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' head:\n",
      "       datum kategorie  wert\n",
      "0 2023-01-12    female    15\n",
      "1 2023-01-23    female    12\n",
      "2 2023-01-31    female    11\n",
      "3 2023-03-25    female    15\n",
      "4 2023-03-27    female    11\n",
      "------------------------------------------------------------\n",
      "'device_usage' head:\n",
      "       datum kategorie     wert\n",
      "0 2022-03-12   desktop   93.000\n",
      "1 2022-03-13   desktop    9.000\n",
      "2 2022-03-14   desktop  610.000\n",
      "3 2022-03-15   desktop    1.381\n",
      "4 2022-03-16   desktop  114.000\n",
      "------------------------------------------------------------\n",
      "'landing_page_views' head:\n",
      "       datum  eid         seitentitel  aufrufe\n",
      "0 2022-03-12    1       New Remix App     92.0\n",
      "1 2022-03-12    2  Application Error!      1.0\n",
      "2 2022-03-13    1       New Remix App      9.0\n",
      "3 2022-03-14    1       New Remix App    609.0\n",
      "4 2022-03-14    2  Application Error!      1.0\n",
      "------------------------------------------------------------\n",
      "'traffic_sources' head:\n",
      "       datum  eid     quelle  sitzungen  aufrufe  aufrufe_pro_sitzung\n",
      "0 2022-03-12    1   (direct)          1   51.000                51.00\n",
      "1 2022-03-12    2  (not set)          1   42.000                42.00\n",
      "2 2022-03-13    1   (direct)          2    9.000                 4.50\n",
      "3 2022-03-14    1   (direct)          9  610.000                67.78\n",
      "4 2022-03-15    1  (not set)          5    1.381               276.20\n",
      "------------------------------------------------------------\n",
      "'traffic_source_chart' head:\n",
      "       datum  kategorie  wert\n",
      "0 2022-03-12   (direct)     1\n",
      "1 2022-03-12  (not set)     1\n",
      "2 2022-03-13   (direct)     1\n",
      "3 2022-03-14   (direct)     2\n",
      "4 2022-03-15  (not set)     0\n",
      "------------------------------------------------------------\n",
      "'user_events' head:\n",
      "       datum  eid name_des_events            event_label  aktive_nutzer  \\\n",
      "0 2023-09-28    1         Checked              (not set)              3   \n",
      "1 2023-09-28    2         Website              (not set)              2   \n",
      "2 2023-09-28    3         Checked             Coming Out              2   \n",
      "3 2023-09-28    4         Checked  Einsamkeit Überwinden              2   \n",
      "4 2023-09-28    5         Checked               Männlich              2   \n",
      "\n",
      "   ereignisanzahl  \n",
      "0              22  \n",
      "1               4  \n",
      "2               5  \n",
      "3               5  \n",
      "4               2  \n",
      "------------------------------------------------------------\n",
      "'user_sessions' head:\n",
      "       datum  seitenaufrufe  nutzer_insgesamt durchschn_zeit_auf_der_seite  \\\n",
      "0 2022-03-12         93.000                 2                     00:01:11   \n",
      "1 2022-03-13          9.000                 2                     00:00:09   \n",
      "2 2022-03-14        610.000                 3                     00:06:29   \n",
      "3 2022-03-15          1.381                 2                     00:05:56   \n",
      "4 2022-03-16        114.000                 2                     00:01:33   \n",
      "\n",
      "   absprungrate  seiten_sitzung  durchschn_zeit_auf_der_seite_seconds  \\\n",
      "0           0.5           46.50                                    71   \n",
      "1           0.0            4.50                                     9   \n",
      "2           0.0           67.78                                   389   \n",
      "3           1.0          276.20                                   356   \n",
      "4           1.0           57.00                                    93   \n",
      "\n",
      "   durchschn_zeit_auf_der_seite_days  \n",
      "0                           0.000822  \n",
      "1                           0.000104  \n",
      "2                           0.004502  \n",
      "3                           0.004120  \n",
      "4                           0.001076  \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' head:\")\n",
    "    print(df.head())\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
