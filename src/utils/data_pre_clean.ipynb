{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d8143887836660",
   "metadata": {},
   "source": [
    "# CSV processing for Redezeit's dashboard scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af22fd9cab8a3f",
   "metadata": {},
   "source": [
    "This notebook's purpose is to process the scraped csv files from Redezeit's Looker Studio dashboard and rename the csv files with a more meaningful name.\n",
    "We preferred a nb instead of a .py file to have a controlled view of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.729124Z",
     "start_time": "2025-06-28T17:14:45.719853Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90caa5863a34e181",
   "metadata": {},
   "source": [
    "### Path, file list, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8956d85e672f5f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.749187Z",
     "start_time": "2025-06-28T17:14:45.743701Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Current nb location\n",
    "BASE_DIR = BASE_DIR = os.getcwd()\n",
    "\n",
    "#  Path to raw data folder\n",
    "folder_path = os.path.normpath(os.path.join(BASE_DIR, '..', 'data', 'raw'))\n",
    "\n",
    "#  Create output folder for cleaned CSV'set\n",
    "clean_folder = os.path.join(BASE_DIR, '..', 'data', 'clean')\n",
    "os.makedirs(clean_folder, exist_ok=True)\n",
    "\n",
    "file_names = [\n",
    "    'landingpage.csv',\n",
    "    'user_behaviors.csv',\n",
    "    'what_devices_used_chart.csv',\n",
    "    'what_did_user_do.csv',\n",
    "    'where_did_they_come_from.csv',\n",
    "    'where_new_visitors_come_from_chart.csv',\n",
    "    'who_was_visiting_chart.csv'\n",
    "]\n",
    "\n",
    "\n",
    "# Standardize final filenames\n",
    "\n",
    "final_names = {\n",
    "    'landingpage.csv':                        'landing_page_views.csv',\n",
    "    'user_behaviors.csv':                     'user_sessions.csv',\n",
    "    'what_devices_used_chart.csv':            'device_usage.csv',\n",
    "    'what_did_user_do.csv':                   'user_events.csv',\n",
    "    'where_did_they_come_from.csv':           'traffic_sources.csv',\n",
    "    'where_new_visitors_come_from_chart.csv': 'traffic_source_chart.csv',\n",
    "    'who_was_visiting_chart.csv':             'daily_visitors_chart.csv'\n",
    "}\n",
    "\n",
    "#  Expected numer of cols in each file (for validation)\n",
    "expected_columns = {\n",
    "    'landingpage.csv':                        3,\n",
    "    'user_behaviors.csv':                     3,\n",
    "    'what_devices_used_chart.csv':            4,\n",
    "    'what_did_user_do.csv':                   6,\n",
    "    'where_did_they_come_from.csv':           3,\n",
    "    'where_new_visitors_come_from_chart.csv': 6,\n",
    "    'who_was_visiting_chart.csv':             6\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd68a0af6ac1ce4",
   "metadata": {},
   "source": [
    "####  Helper: sort by week\n",
    "##### Not ready yet, overwritting everything atm bc i need then ratbros (advice bros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7983938e32bd5de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.765895Z",
     "start_time": "2025-06-28T17:14:45.759620Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef extract_week_folder(path: str) -> str:\\n    \"\"\"\\n    Extracts the ISO calendar week folder name (e.g. \\'2025_KW07\\') based on the first valid\\n    date found in the \\'Datum\\' column of a CSV file.\\n\\n    Args:\\n        path (str): Path to the CSV file.\\n\\n    Returns:\\n        str: Folder name representing year and calendar week, or \"unknown_week\" if not found.\\n    \"\"\"\\n    try:\\n        with open(path, encoding=\"utf-8\") as f:\\n            # Read header line and remove Byte Order Mark (BOM) if present\\n            header_line = f.readline().lstrip(\\'\\ufeff\\')\\n            header = header_line.strip().split(\\';\\')\\n\\n            # Convert header names to lowercase for case-insensitive search\\n            header_lower = [h.lower() for h in header]\\n\\n            # Find the index of the \\'datum\\' column (date column)\\n            datum_idx = header_lower.index(\"datum\")\\n\\n            # Iterate through each line in the file to find a valid date\\n            for line in f:\\n                columns = line.strip().split(\\';\\')\\n\\n                # Skip line if it doesn\\'t have enough columns\\n                if len(columns) <= datum_idx:\\n                    continue\\n\\n                date_str = columns[datum_idx].strip()\\n\\n                try:\\n                    # Parse the date string assuming format YYYY-MM-DD\\n                    dt = datetime.strptime(date_str, \\'%Y-%m-%d\\')\\n\\n                    # Extract year and ISO calendar week number\\n                    year, week, _ = dt.isocalendar()\\n\\n                    # Format the folder name like \\'2025_KW07\\'\\n                    week_folder = f\"{year}_KW{week:02d}\"\\n\\n                    # Confirm success once, then return the folder name\\n                    print(f\"✅ Week folder extracted: {week_folder} from file {path}\")\\n                    return week_folder\\n\\n                except ValueError:\\n                    # If date parsing fails, try next line\\n                    continue\\n\\n    except FileNotFoundError:\\n        print(f\"⚠️ File not found: {path}\")\\n    except Valu\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def extract_week_folder(path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the ISO calendar week folder name (e.g. '2025_KW07') based on the first valid\n",
    "    date found in the 'Datum' column of a CSV file.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        str: Folder name representing year and calendar week, or \"unknown_week\" if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            # Read header line and remove Byte Order Mark (BOM) if present\n",
    "            header_line = f.readline().lstrip('\\ufeff')\n",
    "            header = header_line.strip().split(';')\n",
    "            \n",
    "            # Convert header names to lowercase for case-insensitive search\n",
    "            header_lower = [h.lower() for h in header]\n",
    "            \n",
    "            # Find the index of the 'datum' column (date column)\n",
    "            datum_idx = header_lower.index(\"datum\")\n",
    "            \n",
    "            # Iterate through each line in the file to find a valid date\n",
    "            for line in f:\n",
    "                columns = line.strip().split(';')\n",
    "                \n",
    "                # Skip line if it doesn't have enough columns\n",
    "                if len(columns) <= datum_idx:\n",
    "                    continue\n",
    "                \n",
    "                date_str = columns[datum_idx].strip()\n",
    "                \n",
    "                try:\n",
    "                    # Parse the date string assuming format YYYY-MM-DD\n",
    "                    dt = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                    \n",
    "                    # Extract year and ISO calendar week number\n",
    "                    year, week, _ = dt.isocalendar()\n",
    "                    \n",
    "                    # Format the folder name like '2025_KW07'\n",
    "                    week_folder = f\"{year}_KW{week:02d}\"\n",
    "                    \n",
    "                    # Confirm success once, then return the folder name\n",
    "                    print(f\"✅ Week folder extracted: {week_folder} from file {path}\")\n",
    "                    return week_folder\n",
    "                \n",
    "                except ValueError:\n",
    "                    # If date parsing fails, try next line\n",
    "                    continue\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"⚠️ File not found: {path}\")\n",
    "    except Valu\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c1c15",
   "metadata": {},
   "source": [
    "####  Helper: column count validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be50c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv(path: str, expected_cols: int):\n",
    "    \"\"\"\n",
    "    Scan a semicolon-delimited CSV and report any lines\n",
    "    that don’t have exactly expected_cols fields.\n",
    "    Returns a list of (line_number, text) for the bad lines.\n",
    "    \"\"\"\n",
    "    issues = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for i, line in enumerate(f, start=1):\n",
    "            if line.count(\";\") + 1 != expected_cols:\n",
    "                issues.append((i, line.rstrip(\"\\n\")))\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d60dec775153e",
   "metadata": {},
   "source": [
    "### Morphin'-zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20040b75883ea6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:24:02.002234Z",
     "start_time": "2025-06-28T17:24:01.938901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All files copied, renamed, and validated in clean folder.\n"
     ]
    }
   ],
   "source": [
    "cleaned_files = []\n",
    "\n",
    "for raw_fname in file_names:\n",
    "    raw_path = os.path.join(folder_path, raw_fname)\n",
    "    final_name = final_names.get(raw_fname, raw_fname)\n",
    "\n",
    "    # Ensure destination folder exists\n",
    "    os.makedirs(clean_folder, exist_ok=True)\n",
    "    clean_path = os.path.join(clean_folder, final_name)\n",
    "\n",
    "    # Copy raw file to clean folder\n",
    "    with open(raw_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(clean_path, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(infile.read())\n",
    "\n",
    "    # Validate cleaned file columns count\n",
    "    exp = expected_columns.get(final_name)\n",
    "    if exp:\n",
    "        problems = validate_csv(clean_path, exp)\n",
    "        if problems:\n",
    "            print(f\"⚠️ {final_name} has {len(problems)} malformed lines:\")\n",
    "            for ln, txt in problems[:5]:\n",
    "                print(f\"  line {ln}: {txt}\")\n",
    "\n",
    "    cleaned_files.append(clean_path)\n",
    "\n",
    "print(\"✅ All files copied, renamed, and validated in clean folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f1089672c4939",
   "metadata": {},
   "source": [
    "### 5) Load CSV's into df\n",
    "\n",
    "By loading everything in df's, we make sure things are smooth and working, ready to load into the DB. Converting types here makes sense because this way we can catch errors before we load everything in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4aa942459875a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:32:19.374640Z",
     "start_time": "2025-06-28T17:32:19.285539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅daily_visitors_chart loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\daily_visitors_chart.csv: (632, 3)\n",
      "✅device_usage loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\device_usage.csv: (2852, 3)\n",
      "✅landing_page_views loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\landing_page_views.csv: (19173, 4)\n",
      "✅traffic_sources loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\traffic_sources.csv: (8778, 6)\n",
      "✅traffic_source_chart loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\traffic_source_chart.csv: (8476, 3)\n",
      "✅user_events loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\user_events.csv: (19321, 6)\n",
      "✅user_sessions loaded from c:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\src\\utils\\..\\data\\clean\\user_sessions.csv: (1147, 6)\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "for root, dirs, files in os.walk(clean_folder):\n",
    "    for fname in files:\n",
    "        if fname.endswith(\".csv\"):\n",
    "            path = os.path.join(root, fname)\n",
    "            key = final_names.get(fname, fname.replace(\".csv\", \"\"))\n",
    "            df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "            dataframes[key] = df\n",
    "            print(f\"✅{key} loaded from {path}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37557bae54da",
   "metadata": {},
   "source": [
    "### 6)  Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa8843061463182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:54.572261Z",
     "start_time": "2025-06-28T17:44:54.561240Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_and_cast_columns(df):\n",
    "    \"\"\"\n",
    "    Cleans and casts DataFrame columns:\n",
    "    - Dates to datetime\n",
    "    - Durations to timedelta strings and numeric seconds/days\n",
    "    - Percentages to floats\n",
    "    - Numeric columns cast to int if possible, else float\n",
    "    - Preserve strings as is\n",
    "\n",
    "    Returns errors dict if any issues, else None\n",
    "    \"\"\"\n",
    "\n",
    "    errors = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        is_obj = df[col].dtype == \"object\"\n",
    "        sample = df[col].dropna().astype(str) if is_obj else None\n",
    "\n",
    "        # 1) Date conversion\n",
    "        if \"datum\" in col.lower():\n",
    "            converted = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'datetime_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 2) Duration hh:mm:ss\n",
    "        if is_obj and sample.str.match(r\"^\\d{2}:\\d{2}:\\d{2}$\").all():\n",
    "            td = pd.to_timedelta(df[col], errors=\"coerce\")\n",
    "            failed = td.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'timedelta_failures': failed.sum()}\n",
    "            df[col] = td.astype(str).str[-8:]\n",
    "            df[f\"{col}_seconds\"] = td.dt.total_seconds()\n",
    "            df[f\"{col}_days\"] = td.dt.total_seconds() / 86400\n",
    "            continue\n",
    "\n",
    "        # 3) Percentage conversion\n",
    "        if is_obj and sample.str.match(r\"^[\\d\\.\\,]+\\s*%$\").all():\n",
    "            stripped = sample.str.replace(\"%\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "            converted = pd.to_numeric(stripped, errors=\"coerce\") / 100\n",
    "            failed = converted.isna() & df[col].notna()\n",
    "            if failed.any():\n",
    "                errors[col] = {'percentage_failures': failed.sum()}\n",
    "            df[col] = converted\n",
    "            continue\n",
    "\n",
    "        # 4) Numeric casting for numeric columns or numeric-like strings\n",
    "        if not is_obj:\n",
    "            # Numeric column - check if all integers (no decimals)\n",
    "            if pd.api.types.is_integer_dtype(df[col]):\n",
    "                # Already int, no action needed\n",
    "                continue\n",
    "            elif pd.api.types.is_float_dtype(df[col]):\n",
    "                # Check if float column can be converted to int safely (no decimals)\n",
    "                if (df[col].dropna() % 1 == 0).all():\n",
    "                    df[col] = df[col].astype('Int64')  # nullable integer dtype to keep NaNs\n",
    "                # else keep as float\n",
    "                continue\n",
    "            else:\n",
    "                # Other numeric types, convert to float by default\n",
    "                df[col] = df[col].astype(float)\n",
    "                continue\n",
    "\n",
    "        # Try converting object columns with numeric values\n",
    "        if is_obj:\n",
    "            # Check if numeric-like strings\n",
    "            if sample.str.match(r\"^\\d+(\\.\\d+)?$\").all():\n",
    "                converted = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "                failed = converted.isna() & df[col].notna()\n",
    "                if failed.any():\n",
    "                    errors[col] = {'numeric_failures': failed.sum()}\n",
    "                    df[col] = df[col].astype(str)  # fallback keep as string\n",
    "                else:\n",
    "                    # Cast to int if all whole numbers else float\n",
    "                    if (converted.dropna() % 1 == 0).all():\n",
    "                        df[col] = converted.astype('Int64')\n",
    "                    else:\n",
    "                        df[col] = converted.astype(float)\n",
    "                continue\n",
    "\n",
    "            # Otherwise preserve as string\n",
    "            df[col] = sample\n",
    "\n",
    "    if errors:\n",
    "        return df, errors\n",
    "    else:\n",
    "        print(\"✅ All columns cleaned and cast successfully.\")\n",
    "        return df, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f8dcb7f96e511",
   "metadata": {},
   "source": [
    "### 7) Run them Jewells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec68df953cfc62ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:58.622623Z",
     "start_time": "2025-06-28T17:44:58.541023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧙🏽‍♂️ ...morphin' DataFrame: 'daily_visitors_chart'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'device_usage'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'landing_page_views'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'traffic_sources'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'traffic_source_chart'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'user_events'\n",
      "✅ All columns cleaned and cast successfully.\n",
      "🧙🏽‍♂️ ...morphin' DataFrame: 'user_sessions'\n",
      "✅ All columns cleaned and cast successfully.\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"🧙🏽‍♂️ ...morphin' DataFrame: '{name}'\")\n",
    "    cleaned_df, error_info = clean_and_cast_columns(df)\n",
    "    dataframes[name] = cleaned_df\n",
    "    if error_info:\n",
    "        print(f\"Errors in '{name}': {error_info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2aacaf93ee925",
   "metadata": {},
   "source": [
    "### 8) Datatype check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bca6e553b3aa9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:14.475854Z",
     "start_time": "2025-06-28T17:45:14.417200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'device_usage' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'landing_page_views' data types:\n",
      "datum          datetime64[ns]\n",
      "eid                     Int64\n",
      "seitentitel            object\n",
      "aufrufe               float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_sources' data types:\n",
      "datum                  datetime64[ns]\n",
      "eid                             Int64\n",
      "quelle                         object\n",
      "sitzungen                       int64\n",
      "aufrufe                       float64\n",
      "aufrufe pro sitzung           float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_source_chart' data types:\n",
      "datum        datetime64[ns]\n",
      "kategorie            object\n",
      "wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_events' data types:\n",
      "datum              datetime64[ns]\n",
      "eid                         Int64\n",
      "name des events            object\n",
      "event_label                object\n",
      "aktive nutzer               int64\n",
      "ereignisanzahl              int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_sessions' data types:\n",
      "datum                            datetime64[ns]\n",
      "seitenaufrufe                           float64\n",
      "nutzer insgesamt                          int64\n",
      "durchschn. zeit auf der seite            object\n",
      "absprungrate                            float64\n",
      "seiten / sitzung                        float64\n",
      "dtype: object\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' data types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e85f2c306a863",
   "metadata": {},
   "source": [
    "### 9) Head check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "338042d6c58b5ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:16.559945Z",
     "start_time": "2025-06-28T17:45:16.545189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' head:\n",
      "       datum kategorie  wert\n",
      "0 2023-01-12    female    15\n",
      "1 2023-01-23    female    12\n",
      "2 2023-01-31    female    11\n",
      "3 2023-03-25    female    15\n",
      "4 2023-03-27    female    11\n",
      "------------------------------------------------------------\n",
      "'device_usage' head:\n",
      "       datum kategorie     wert\n",
      "0 2022-03-12   desktop   93.000\n",
      "1 2022-03-13   desktop    9.000\n",
      "2 2022-03-14   desktop  610.000\n",
      "3 2022-03-15   desktop    1.381\n",
      "4 2022-03-16   desktop  114.000\n",
      "------------------------------------------------------------\n",
      "'landing_page_views' head:\n",
      "       datum  eid         seitentitel  aufrufe\n",
      "0 2022-03-12    1       New Remix App     92.0\n",
      "1 2022-03-12    2  Application Error!      1.0\n",
      "2 2022-03-13    1       New Remix App      9.0\n",
      "3 2022-03-14    1       New Remix App    609.0\n",
      "4 2022-03-14    2  Application Error!      1.0\n",
      "------------------------------------------------------------\n",
      "'traffic_sources' head:\n",
      "       datum  eid     quelle  sitzungen  aufrufe  aufrufe pro sitzung\n",
      "0 2022-03-12    1   (direct)          1   51.000                51.00\n",
      "1 2022-03-12    2  (not set)          1   42.000                42.00\n",
      "2 2022-03-13    1   (direct)          2    9.000                 4.50\n",
      "3 2022-03-14    1   (direct)          9  610.000                67.78\n",
      "4 2022-03-15    1  (not set)          5    1.381               276.20\n",
      "------------------------------------------------------------\n",
      "'traffic_source_chart' head:\n",
      "       datum  kategorie  wert\n",
      "0 2022-03-12   (direct)     1\n",
      "1 2022-03-12  (not set)     1\n",
      "2 2022-03-13   (direct)     1\n",
      "3 2022-03-14   (direct)     2\n",
      "4 2022-03-15  (not set)     0\n",
      "------------------------------------------------------------\n",
      "'user_events' head:\n",
      "       datum  eid name des events            event_label  aktive nutzer  \\\n",
      "0 2023-09-28    1         Checked              (not set)              3   \n",
      "1 2023-09-28    2         Website              (not set)              2   \n",
      "2 2023-09-28    3         Checked             Coming Out              2   \n",
      "3 2023-09-28    4         Checked  Einsamkeit Überwinden              2   \n",
      "4 2023-09-28    5         Checked               Männlich              2   \n",
      "\n",
      "   ereignisanzahl  \n",
      "0              22  \n",
      "1               4  \n",
      "2               5  \n",
      "3               5  \n",
      "4               2  \n",
      "------------------------------------------------------------\n",
      "'user_sessions' head:\n",
      "       datum  seitenaufrufe  nutzer insgesamt durchschn. zeit auf der seite  \\\n",
      "0 2022-03-12         93.000                 2                      00:01:11   \n",
      "1 2022-03-13          9.000                 2                      00:00:09   \n",
      "2 2022-03-14        610.000                 3                      00:06:29   \n",
      "3 2022-03-15          1.381                 2                      00:05:56   \n",
      "4 2022-03-16        114.000                 2                      00:01:33   \n",
      "\n",
      "   absprungrate  seiten / sitzung  \n",
      "0           0.5             46.50  \n",
      "1           0.0              4.50  \n",
      "2           0.0             67.78  \n",
      "3           1.0            276.20  \n",
      "4           1.0             57.00  \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' head:\")\n",
    "    print(df.head())\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00fef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
