{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d8143887836660",
   "metadata": {},
   "source": [
    "# CSV processing for Redezeit's dashboard scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9af22fd9cab8a3f",
   "metadata": {},
   "source": [
    "This notebook's purpose is to process the scraped csv files from Redezeit's Looker Studio dashboard and rename the csv files with a more meaningful name.\n",
    "We preferred a nb instead of a .py file to have a controlled view of each line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.729124Z",
     "start_time": "2025-06-28T17:14:45.719853Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90caa5863a34e181",
   "metadata": {},
   "source": [
    "### Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8956d85e672f5f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.749187Z",
     "start_time": "2025-06-28T17:14:45.743701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup paths and file names\n",
    "BASE_DIR = BASE_DIR = os.getcwd()  # Gets the current working directory in a notebook\n",
    "folder_path = os.path.join(BASE_DIR, '..', 'data')       #  Build relative path\n",
    "folder_path = os.path.normpath(folder_path)     #  Clean up '..' for cross-platform compatibility\n",
    "\n",
    "file_names = [\n",
    "    'landingpage.csv',\n",
    "    'user_behaviors.csv',\n",
    "    'what_devices_used_chart.csv',\n",
    "    'what_did_user_do.csv',\n",
    "    'where_did_they_come_from.csv',\n",
    "    'where_new_visitors_come_from_chart.csv',\n",
    "    'who_was_visiting_chart.csv'\n",
    "]\n",
    "\n",
    "# create output folder for reworked CSVs\n",
    "parent_folder = os.path.dirname(folder_path)  # go up one level\n",
    "clean_folder = os.path.join(folder_path, \"clean\")\n",
    "os.makedirs(clean_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd68a0af6ac1ce4",
   "metadata": {},
   "source": [
    "###  1) Precleaning:\n",
    "\n",
    "Replace csv separator ',' with ';', and '.' to ',' to avoid formatting issues (',' is used as tex format for decimals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7983938e32bd5de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:14:45.765895Z",
     "start_time": "2025-06-28T17:14:45.759620Z"
    }
   },
   "outputs": [],
   "source": [
    "#  pre-cleaning function (replace ',' with ';' and '.' with ',')\n",
    "\n",
    "def pre_clean_file(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "        content = infile.read()\n",
    "\n",
    "    #  step by step replace: 1) comma separation to semicolon\n",
    "    content = content.replace(',', ';')\n",
    "\n",
    "    #  2) decimal dots to commas\n",
    "    content = content.replace ('.', ',')\n",
    "\n",
    "    #  Save\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        outfile.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe51d8ab51d28a2",
   "metadata": {},
   "source": [
    "### 2)  Industry standard file naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "325e83ff6a6301f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:18:31.470030Z",
     "start_time": "2025-06-28T17:18:31.461446Z"
    }
   },
   "outputs": [],
   "source": [
    "load_renames = {\n",
    "    \"cleaned_landingpage.csv\":                \"landing_page_views\",\n",
    "    \"cleaned_user_behaviors.csv\":             \"user_sessions\",\n",
    "    \"cleaned_what_devices_used_chart.csv\":    \"device_usage\",\n",
    "    \"cleaned_what_did_user_do.csv\":           \"user_events\",\n",
    "    \"cleaned_where_did_they_come_from.csv\":   \"traffic_sources\",\n",
    "    \"cleaned_where_they_come_from_chart.csv\": \"traffic_source_chart\",\n",
    "    \"cleaned_who_was_visiting_chart.csv\":     \"daily_visitors_chart\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d60dec775153e",
   "metadata": {},
   "source": [
    "### 3) Apply pre-cleaning and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "20040b75883ea6da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:24:02.002234Z",
     "start_time": "2025-06-28T17:24:01.938901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files prepared and saved with renamed filenames.\n"
     ]
    }
   ],
   "source": [
    "# Apply pre-cleaning and save with new names\n",
    "cleaned_files = []\n",
    "for file in file_names:\n",
    "    raw_path = os.path.join(folder_path, file)\n",
    "    cleaned_name = f\"cleaned_{file}\"\n",
    "\n",
    "    # Use renamed filename if available\n",
    "    final_name = load_renames.get(cleaned_name, cleaned_name) + \".csv\"\n",
    "    clean_path = os.path.join(clean_folder, final_name)\n",
    "\n",
    "    pre_clean_file(raw_path, clean_path)\n",
    "    cleaned_files.append(clean_path)\n",
    "\n",
    "print(\"Files prepared and saved with renamed filenames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f1089672c4939",
   "metadata": {},
   "source": [
    "### 5) Load CSV's into df\n",
    "\n",
    "By loading everything in df's, we make sure things are smooth and working, ready to load into the DB. Converting types here makes sense because this way we can catch errors before we load everything in Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4aa942459875a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:32:19.374640Z",
     "start_time": "2025-06-28T17:32:19.285539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned_where_new_visitors_come_from_chart loaded from cleaned_where_new_visitors_come_from_chart.csv.csv: (7302, 3)\n",
      "daily_visitors_chart loaded from daily_visitors_chart.csv: (636, 3)\n",
      "device_usage loaded from device_usage.csv: (2293, 3)\n",
      "landing_page_views loaded from landing_page_views.csv: (16804, 4)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 6 fields in line 7526, saw 11\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m path = os.path.join(clean_folder, fname)\n\u001b[32m      6\u001b[39m key = load_renames.get(fname, fname.replace(\u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m dataframes[key] = df\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\Data_Craft_2024-25\\Projects\\Redezeit\\Redezeit-Analyse\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 6 fields in line 7526, saw 11\n"
     ]
    }
   ],
   "source": [
    "# Load cleaned files using new names\n",
    "dataframes = {}\n",
    "for fname in os.listdir(clean_folder):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        path = os.path.join(clean_folder, fname)\n",
    "        key = load_renames.get(fname, fname.replace(\".csv\", \"\"))\n",
    "        df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "        dataframes[key] = df\n",
    "        print(f\"{key} loaded from {fname}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed37557bae54da",
   "metadata": {},
   "source": [
    "### 6)  Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eaa8843061463182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:54.572261Z",
     "start_time": "2025-06-28T17:44:54.561240Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_all_columns(df):\n",
    "    \"\"\"\n",
    "Cleans and standardizes all columns in a pandas DataFrame by converting dates, durations,percentages, and localized numeric formats into consistent types.\n",
    "\n",
    "Args:\n",
    "    df: A pandas DataFrame containing raw, string-based or mixed-type columns.\n",
    "\n",
    "Returns:\n",
    "    -Pandas DataFrame with cleaned columns.\n",
    "    -Datetime columns are parsed, duration columns are converted to time strings and numeric values\n",
    "    -Percentages are normalized to floats.\n",
    "    -German-style numeric strings are converted to float values.\n",
    "    -All other string columns are preserved as-is.\n",
    "\n",
    "Raises:\n",
    "    None explicitly, but columns with unsupported formats may result in NaNs due to coercion.\n",
    "\"\"\"\n",
    "    # Define fields to skip numeric conversion\n",
    "    text_fields = {\"name des events\", \"even_label\", \"kategorie\", \"quelle\", \"source\"}\n",
    "\n",
    "    for col in df.columns:\n",
    "        is_obj = df[col].dtype == \"object\"\n",
    "        sample = df[col].dropna().astype(str) if is_obj else None\n",
    "\n",
    "        # 1) Date conversion\n",
    "        if \"datum\" in col.lower():\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "            print(f\"📅 Converted to datetime: {col}\")\n",
    "            continue\n",
    "\n",
    "        # 2) Time durations hh:mm:ss\n",
    "        if is_obj and sample.str.match(r\"^\\d{2}:\\d{2}:\\d{2}$\").all():\n",
    "            td = pd.to_timedelta(df[col], errors=\"coerce\")\n",
    "            # human-readable HH:MM:SS\n",
    "            df[col] = td.astype(str).str[-8:]\n",
    "            # numeric columns\n",
    "            df[f\"{col}_seconds\"] = td.dt.total_seconds()\n",
    "            df[f\"{col}_days\"]    = td.dt.total_seconds() / 86400\n",
    "            print(f\"⏱️ Processed time column: {col}\")\n",
    "            continue\n",
    "\n",
    "        # 3) Percentage conversion (only pure percent strings)\n",
    "        if is_obj and sample.str.match(r\"^[\\d\\.\\,]+\\s*%$\").all():\n",
    "            df[col] = (\n",
    "                sample.str.replace(\"%\", \"\", regex=False)\n",
    "                      .str.replace(\",\", \".\", regex=False)\n",
    "                      .astype(float) / 100\n",
    "            )\n",
    "            print(f\"📊 Converted percentage: {col}\")\n",
    "            continue\n",
    "\n",
    "        # 4) German-style numeric conversion\n",
    "        if is_obj and col.lower() not in text_fields:\n",
    "            # check pure numeric patterns\n",
    "            if sample.str.match(r\"^[\\d\\.\\,]+$\").all():\n",
    "                cleaned = sample.str.replace(\".\", \"\", regex=False)  # remove thousands\n",
    "                cleaned = cleaned.str.replace(\",\", \".\", regex=False)  # decimal comma\n",
    "                df[col] = pd.to_numeric(cleaned, errors=\"coerce\")\n",
    "                print(f\"🔢 Converted to numeric: {col}\")\n",
    "                continue\n",
    "\n",
    "        # 5) Preserve any other object columns as strings\n",
    "        if is_obj:\n",
    "            df[col] = sample\n",
    "            print(f\"🔤 Preserved string column: {col}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f8dcb7f96e511",
   "metadata": {},
   "source": [
    "### 7) Run them Jewells!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec68df953cfc62ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:44:58.622623Z",
     "start_time": "2025-06-28T17:44:58.541023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning DataFrame: daily_visitors_chart\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Kategorie\n",
      "Cleaning DataFrame: device_usage\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Kategorie\n",
      "Cleaning DataFrame: landing_page_views\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Seitentitel\n",
      "Cleaning DataFrame: traffic_sources\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Quelle\n",
      "Cleaning DataFrame: traffic_source_chart\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Kategorie\n",
      "Cleaning DataFrame: user_events\n",
      "📅 Converted to datetime: Datum\n",
      "🔤 Preserved string column: Name des Events\n",
      "🔤 Preserved string column: even_label\n",
      "Cleaning DataFrame: user_sessions\n",
      "📅 Converted to datetime: Datum\n",
      "⏱️ Processed time column: Durchschn, Zeit auf der Seite\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"Cleaning DataFrame: {name}\")\n",
    "    dataframes[name] = clean_all_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc2aacaf93ee925",
   "metadata": {},
   "source": [
    "### 8) Datatype check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bca6e553b3aa9a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:14.475854Z",
     "start_time": "2025-06-28T17:45:14.417200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' data types:\n",
      "Datum        datetime64[ns]\n",
      "Kategorie            object\n",
      "Wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'device_usage' data types:\n",
      "Datum        datetime64[ns]\n",
      "Kategorie            object\n",
      "Wert                float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'landing_page_views' data types:\n",
      "Datum          datetime64[ns]\n",
      "EID                   float64\n",
      "Seitentitel            object\n",
      "Aufrufe               float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_sources' data types:\n",
      "Datum                  datetime64[ns]\n",
      "EID                           float64\n",
      "Quelle                         object\n",
      "Sitzungen                       int64\n",
      "Aufrufe                       float64\n",
      "Aufrufe pro Sitzung           float64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'traffic_source_chart' data types:\n",
      "Datum        datetime64[ns]\n",
      "Kategorie            object\n",
      "Wert                  int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_events' data types:\n",
      "Datum              datetime64[ns]\n",
      "EID                       float64\n",
      "Name des Events            object\n",
      "even_label                 object\n",
      "Aktive Nutzer               int64\n",
      "Ereignisanzahl              int64\n",
      "dtype: object\n",
      "----------------------------------------\n",
      "'user_sessions' data types:\n",
      "Datum                                    datetime64[ns]\n",
      "Seitenaufrufe                                   float64\n",
      "Nutzer Insgesamt                                  int64\n",
      "Durchschn, Zeit auf der Seite                    object\n",
      "Absprungrate                                    float64\n",
      "Seiten / Sitzung                                float64\n",
      "Durchschn, Zeit auf der Seite_seconds           float64\n",
      "Durchschn, Zeit auf der Seite_days              float64\n",
      "dtype: object\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' data types:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e85f2c306a863",
   "metadata": {},
   "source": [
    "### 9) Head check for each df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "338042d6c58b5ee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T17:45:16.559945Z",
     "start_time": "2025-06-28T17:45:16.545189Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'daily_visitors_chart' head:\n",
      "       Datum Kategorie  Wert\n",
      "0 2023-01-12    female    15\n",
      "1 2023-01-23    female    12\n",
      "2 2023-01-31    female    11\n",
      "3 2023-03-25    female    15\n",
      "4 2023-03-27    female    11\n",
      "------------------------------------------------------------\n",
      "'device_usage' head:\n",
      "       Datum Kategorie  Wert\n",
      "0 2023-02-12    mobile  49.0\n",
      "1 2023-02-12   desktop  48.0\n",
      "2 2023-02-13   desktop  64.0\n",
      "3 2023-02-13    mobile  36.0\n",
      "4 2023-02-14   desktop  63.0\n",
      "------------------------------------------------------------\n",
      "'landing_page_views' head:\n",
      "       Datum  EID                                        Seitentitel  Aufrufe\n",
      "0 2023-01-01  1.0  REDEZEIT FÜR DICH Start DE | REDEZEIT FÜR DICH...     11.0\n",
      "1 2023-01-01  2.0  Ich biete Redezeit, | REDEZEIT FÜR DICH #virtu...      2.0\n",
      "2 2023-01-01  3.0  Lesezeit – das Redezeit Blog, | REDEZEIT FÜR D...      2.0\n",
      "3 2023-01-01  4.0  SEO WER SIND WIR DE | REDEZEIT FÜR DICH #virtu...      2.0\n",
      "4 2023-01-01  5.0  Du suchst REDEZEIT für Dich DE | REDEZEIT FÜR ...      1.0\n",
      "------------------------------------------------------------\n",
      "'traffic_sources' head:\n",
      "       Datum  EID                   Quelle  Sitzungen  Aufrufe  \\\n",
      "0 2023-01-01  1.0                   google          5     18.0   \n",
      "1 2023-01-01  2.0                 (direct)          1      1.0   \n",
      "2 2023-01-01  3.0  seelischegesundheit,net          1      1.0   \n",
      "3 2023-01-02  1.0                 (direct)          7    394.0   \n",
      "4 2023-01-02  2.0                   google          7     23.0   \n",
      "\n",
      "   Aufrufe pro Sitzung  \n",
      "0                 3.60  \n",
      "1                 1.00  \n",
      "2                 1.00  \n",
      "3                56.29  \n",
      "4                 3.29  \n",
      "------------------------------------------------------------\n",
      "'traffic_source_chart' head:\n",
      "       Datum                Kategorie  Wert\n",
      "0 2023-01-01                   google     4\n",
      "1 2023-01-01                 (direct)     1\n",
      "2 2023-01-01  seelischegesundheit,net     1\n",
      "3 2023-01-02                   google     5\n",
      "4 2023-01-02                 (direct)     3\n",
      "------------------------------------------------------------\n",
      "'user_events' head:\n",
      "       Datum  EID Name des Events             even_label  Aktive Nutzer  \\\n",
      "0 2023-09-28  1.0         Checked              (not set)              3   \n",
      "1 2023-09-28  2.0         Website              (not set)              2   \n",
      "2 2023-09-28  3.0         Checked             Coming Out              2   \n",
      "3 2023-09-28  4.0         Checked  Einsamkeit Überwinden              2   \n",
      "4 2023-09-28  5.0         Checked               Männlich              2   \n",
      "\n",
      "   Ereignisanzahl  \n",
      "0              22  \n",
      "1               4  \n",
      "2               5  \n",
      "3               5  \n",
      "4               2  \n",
      "------------------------------------------------------------\n",
      "'user_sessions' head:\n",
      "       Datum  Seitenaufrufe  Nutzer Insgesamt Durchschn, Zeit auf der Seite  \\\n",
      "0 2023-01-01           20.0                 6                      00:01:02   \n",
      "1 2023-01-02          434.0                15                      00:04:38   \n",
      "2 2023-01-03          120.0                18                      00:02:47   \n",
      "3 2023-01-04          284.0                32                      00:01:54   \n",
      "4 2023-01-05          143.0                30                      00:02:08   \n",
      "\n",
      "   Absprungrate  Seiten / Sitzung  Durchschn, Zeit auf der Seite_seconds  \\\n",
      "0        0.0000              2.86                                   62.0   \n",
      "1        0.0556             24.11                                  278.0   \n",
      "2        0.0476              5.71                                  167.0   \n",
      "3        0.1750              7.10                                  114.0   \n",
      "4        0.0263              3.76                                  128.0   \n",
      "\n",
      "   Durchschn, Zeit auf der Seite_days  \n",
      "0                            0.000718  \n",
      "1                            0.003218  \n",
      "2                            0.001933  \n",
      "3                            0.001319  \n",
      "4                            0.001481  \n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for name, df in dataframes.items():\n",
    "    print(f\"'{name}' head:\")\n",
    "    print(df.head())\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af4dde3c84403f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-26T08:14:22.782410Z",
     "start_time": "2025-06-26T08:14:22.779407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20;19,;Über uns | REDEZEIT FÜR DICH #virtualsupporttalks;02025-06-21;1,;Willkommen | REDEZEIT FÜR DICH #virtualsupporttalks;14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path, encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "    print(lines[16634])  # Python uses 0-based indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37495591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-27;9,;duckduckgo;1;0;02025-06-28;1,;google;31;30;0,97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(path, encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i == 7525:  # line number is zero-indexed\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230008c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
